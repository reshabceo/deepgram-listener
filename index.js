require('dotenv').config();

const express = require('express');
const expressWs = require('express-ws');
const WebSocket = require('ws');
const axios = require('axios');
const { createClient } = require('@supabase/supabase-js');
const fetch = require('node-fetch');

// Validate required environment variables
const requiredEnvVars = [
  'SUPABASE_URL',
  'SUPABASE_SERVICE_KEY',
  'DEEPGRAM_API_KEY',
  'HUGGING_FACE_API_KEY'
];

for (const envVar of requiredEnvVars) {
  if (!process.env[envVar]) {
    console.error(`‚ùå Missing required environment variable: ${envVar}`);
    process.exit(1);
  }
}

const app = express();
const wsInstance = expressWs(app);

const port = process.env.PORT || 3000;
const KEEP_ALIVE_INTERVAL = 30000; // 30 seconds
const PROCESSING_TIMEOUT = 60000; // 60 seconds
const RATE_LIMIT_WINDOW = 60000; // 1 minute
const MAX_REQUESTS_PER_WINDOW = 50;
const requestTimestamps = [];
const MAX_RECONNECT_ATTEMPTS = 3;
const RECONNECT_DELAY = 2000; // 2 seconds
const CONNECTION_TIMEOUT = 5000; // 5 seconds
const HUGGING_FACE_API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-Small-3.1-24B-Instruct-2503";

// Supabase client
const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_KEY,
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false
    }
  }
);

// Add after the OpenAI initialization
const FALLBACK_RESPONSES = [
  "I'm listening. Please continue.",
  "I understand. Go on.",
  "Could you tell me more about that?",
  "I'm here to help. What else would you like to share?",
  "I'm following. Please continue.",
  "That's interesting. Would you like to elaborate?",
  "I see. What are your thoughts on that?",
  "Thank you for sharing. Would you like to continue?",
];

// Conversation context management
class ConversationManager {
  constructor() {
    this.contexts = new Map();
    this.metrics = new Map();
  }

  async initializeContext(callId) {
    console.log("üéØ Initializing context for call:", callId);
    
    const context = {
      messages: [{
        role: "system",
        content: `You are a helpful phone assistant. Keep responses brief and natural. Be professional and focused.`
      }],
      lastProcessedTime: Date.now(),
      transcriptBuffer: '',
      silenceCount: 0,
      startTime: Date.now()
    };

    this.contexts.set(callId, context);
    
    // Initialize metrics with timestamps
    this.metrics.set(callId, {
      startTime: Date.now(),
      userSpeakingTime: 0,
      aiResponseTime: 0,
      silenceTime: 0,
      turnCount: 0,
      responseTimes: [],
      lastMetricUpdate: Date.now()
    });
    
    try {
      // Store conversation start in Supabase
      await supabase.from('conversations').insert([{
        call_id: callId,
        start_time: new Date().toISOString(),
        status: 'active'
      }]);
      console.log("üíæ Initialized conversation in database");
    } catch (error) {
      console.error('‚ùå Failed to initialize conversation in database:', error);
      throw error;
    }
  }

  getContext(callId) {
    return this.contexts.get(callId);
  }

  async updateContext(callId, message) {
    const context = this.getContext(callId);
    if (context) {
      context.messages.push(message);
      // Keep context window manageable
      if (context.messages.length > 10) {
        context.messages = [
          context.messages[0],
          ...context.messages.slice(-9)
        ];
      }
    }
  }

  updateMetrics(callId, type, duration) {
    console.log(`üìä Updating metrics for ${callId} - Type: ${type}, Duration: ${duration}ms`);
    
    const metrics = this.metrics.get(callId);
    if (metrics) {
      const now = Date.now();
      
      switch (type) {
        case 'user_speaking':
          metrics.userSpeakingTime += duration;
          break;
        case 'ai_response':
          metrics.aiResponseTime += duration;
          break;
        case 'silence':
          metrics.silenceTime += duration;
          break;
        case 'response_time':
          metrics.responseTimes.push(duration);
          metrics.turnCount++;
          break;
      }
      
      metrics.lastMetricUpdate = now;
      console.log(`üìä Updated metrics for ${callId}:`, metrics);
    } else {
      console.error(`‚ùå No metrics found for call: ${callId}`);
    }
  }

  async endConversation(callId) {
    console.log(`üîö Ending conversation for call: ${callId}`);
    
    try {
      const metrics = this.metrics.get(callId);
      const endTime = Date.now();
      
      if (metrics) {
        const totalDuration = endTime - metrics.startTime;
        const avgResponseTime = metrics.responseTimes.length > 0 
          ? metrics.responseTimes.reduce((a, b) => a + b, 0) / metrics.responseTimes.length 
          : 0;

        console.log(`üìä Final metrics for ${callId}:`, {
          totalDuration,
          userSpeakingTime: metrics.userSpeakingTime,
          aiResponseTime: metrics.aiResponseTime,
          silenceTime: metrics.silenceTime,
          turnCount: metrics.turnCount,
          avgResponseTime
        });

        // Update conversation status
        await supabase.from('conversations')
          .update({
            end_time: new Date().toISOString(),
            status: 'completed'
          })
          .eq('call_id', callId);

        // Store call metrics
        await supabase.from('call_metrics').insert([{
          call_id: callId,
          total_duration: totalDuration,
          user_speaking_time: metrics.userSpeakingTime,
          ai_response_time: metrics.aiResponseTime,
          silence_time: metrics.silenceTime,
          turn_count: metrics.turnCount,
          average_response_time: avgResponseTime
        }]);
        
        console.log("üíæ Stored final metrics in database");
      }

      // Clean up
      this.contexts.delete(callId);
      this.metrics.delete(callId);
    } catch (error) {
      console.error('‚ùå Error ending conversation:', error);
    }
  }
}

const conversationManager = new ConversationManager();

// Enhanced text processing utilities
const textUtils = {
  isFiller: (text) => {
    const fillerWords = new Set([
      'uh', 'umm', 'hmm', 'ah', 'eh', 'like', 'you know', 
      'well', 'so', 'basically', 'actually', 'literally'
    ]);
    return fillerWords.has(text.toLowerCase().trim());
  },

  isEndOfThought: (text, timeSinceLast) => {
    // Natural pauses
    if (timeSinceLast > 1500) return true;
    
    // Punctuation
    if (/[.!?]$/.test(text.trim())) return true;
    
    // Common ending phrases
    const endPhrases = ['okay', 'right', 'you see', 'you know what i mean', 'thank you'];
    return endPhrases.some(phrase => text.toLowerCase().trim().endsWith(phrase));
  },

  cleanTranscript: (text) => {
    return text
      .replace(/\s+/g, ' ')
      .trim()
      .replace(/(\w)gonna(\w)?/g, '$1going to$2')
      .replace(/(\w)wanna(\w)?/g, '$1want to$2')
      .replace(/(\w)dunno(\w)?/g, '$1don\'t know$2');
  },

  hasMinimumQuality: (text) => {
    const words = text.split(/\s+/);
    return words.length >= 2 && text.length >= 5;
  }
};

// Add rate limiting function
function checkRateLimit() {
  const now = Date.now();
  // Remove timestamps older than the window
  while (requestTimestamps.length > 0 && requestTimestamps[0] < now - RATE_LIMIT_WINDOW) {
    requestTimestamps.shift();
  }
  // Check if we're under the limit
  if (requestTimestamps.length < MAX_REQUESTS_PER_WINDOW) {
    requestTimestamps.push(now);
    return true;
  }
  return false;
}

// Update the TTS response format and add error handling
const sendTTSResponse = async (ws, text) => {
  try {
    // Clean and format the text for TTS
    const cleanText = text.replace(/[<>]/g, '').trim();
    const ttsResponse = `<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Speak voice="Polly.Joanna" language="en-US">${cleanText}</Speak>
</Response>`;
    
    console.log("üéØ WebSocket State:", ws.readyState);
    console.log("üìù TTS XML:", ttsResponse);
    
    if (ws.readyState === WebSocket.OPEN) {
      const message = {
        event: 'speak',
        payload: ttsResponse
      };
      console.log("üì§ Sending message to Plivo:", message);
      ws.send(JSON.stringify(message));
      
      // Add message handler for Plivo responses
      const messageHandler = (response) => {
        try {
          const parsed = JSON.parse(response.toString());
          console.log("üì• Plivo response event:", parsed.event);
          
          if (parsed.event === 'speak') {
            console.log("üîä TTS speak event received:", parsed);
          } else if (parsed.event === 'media') {
            // Ignore media events as they're for audio streaming
          } else {
            console.log("‚ÑπÔ∏è Other Plivo event:", parsed.event);
          }
        } catch (err) {
          console.error("‚ùå Error parsing Plivo response:", err);
        }
      };
      
      // Listen for the next few messages to catch the speak response
      for (let i = 0; i < 5; i++) {
        ws.once('message', messageHandler);
      }
    } else {
      throw new Error(`WebSocket not open (State: ${ws.readyState})`);
    }
  } catch (error) {
    console.error("‚ùå Error sending TTS response:", error);
  }
};

// Enhanced ChatGPT integration
async function generateAIResponse(callId, userMessage) {
  const context = conversationManager.getContext(callId);
  if (!context) return null;

  const startTime = Date.now();

  try {
    // Check rate limit before making API call
    if (!checkRateLimit()) {
      console.log("‚ö†Ô∏è Rate limit reached, using fallback response");
      return "I apologize, but I'm receiving too many requests right now. Could you please repeat that?";
    }

    // Add user message to context
    await conversationManager.updateContext(callId, {
      role: "user",
      content: userMessage
    });

    let aiResponse;
    
    try {
      console.log("ü§ñ Using Hugging Face for response generation");
      const response = await fetch(HUGGING_FACE_API_URL, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${process.env.HUGGING_FACE_API_KEY}`
        },
        body: JSON.stringify({
          inputs: `<s>[INST] ${userMessage} [/INST]`,
          parameters: {
            max_new_tokens: 100,
            temperature: 0.7,
            top_p: 0.9,
            return_full_text: false,
            do_sample: true
          }
        }),
      });

      if (!response.ok) {
        throw new Error(`Hugging Face API error: ${response.status} - ${await response.text()}`);
      }

      const result = await response.json();
      aiResponse = result[0]?.generated_text || FALLBACK_RESPONSES[0];
      console.log("ü§ñ Hugging Face response:", aiResponse);
    } catch (error) {
      console.error("‚ùå Hugging Face API error:", error);
      // Use fallback if Hugging Face fails
      aiResponse = FALLBACK_RESPONSES[Math.floor(Math.random() * FALLBACK_RESPONSES.length)];
    }

    const responseTime = Date.now() - startTime;
    
    // Update metrics
    conversationManager.updateMetrics(callId, 'response_time', responseTime);
    
    // Add AI response to context
    await conversationManager.updateContext(callId, {
      role: "assistant",
      content: aiResponse
    });

    try {
      await supabase.from('conversation_turns').insert([{
        call_id: callId,
        user_message: userMessage,
        ai_response: aiResponse,
        is_hugging_face: true,
        timestamp: new Date().toISOString()
      }]);
    } catch (dbError) {
      console.error('‚ùå Failed to store conversation turn:', dbError);
    }

    return aiResponse;

  } catch (error) {
    console.error('‚ùå General error in generateAIResponse:', error);
    return "I apologize, but I'm experiencing technical difficulties. Please try again.";
  }
}

// ‚úÖ For Railway status check
app.get('/', (req, res) => {
  res.send('‚úÖ Deepgram Listener is running');
});

// ‚úÖ Serve Plivo XML
app.get('/plivo-xml', (req, res) => {
  const xml = `<?xml version="1.0" encoding="UTF-8"?>
  <Response>
    <Record 
      action="https://bms123.app.n8n.cloud/webhook/recording"
      redirect="false"
      recordSession="true"
      maxLength="7200"
      startOnDialAnswer="true" />
    <Stream 
      streamTimeout="7200"
      keepCallAlive="true"
      bidirectional="true"
      contentType="audio/x-mulaw;rate=8000"
      statusCallbackUrl="https://bms123.app.n8n.cloud/webhook/stream-status">
      wss://triumphant-victory-production.up.railway.app/listen
    </Stream>
  </Response>`;
  res.set('Content-Type', 'text/xml');
  res.send(xml);
});

// Constants for API configuration
const DEEPGRAM_CONFIG = {
  encoding: 'mulaw',
  sample_rate: 8000,
  channels: 1,
  model: 'general',
  language: 'en-US',
  punctuate: true
};

// Initialize Deepgram WebSocket with connection handling
async function initializeDeepgramWebSocket() {
  return new Promise((resolve, reject) => {
    console.log('üéôÔ∏è Initializing Deepgram connection...');
    const wsUrl = `wss://api.deepgram.com/v1/listen?${new URLSearchParams(DEEPGRAM_CONFIG).toString()}`;
    console.log('üîó Connecting to:', wsUrl);

    const ws = new WebSocket(wsUrl, {
      headers: {
        Authorization: `Token ${process.env.DEEPGRAM_API_KEY}`
      }
    });

    const connectionTimeout = setTimeout(() => {
      if (ws.readyState !== WebSocket.OPEN) {
        console.error('‚ùå Deepgram connection timeout');
        ws.close();
        reject(new Error('Connection timeout'));
      }
    }, 5000);

    ws.on('open', () => {
      console.log('‚úÖ Deepgram WebSocket connected');
      clearTimeout(connectionTimeout);
      resolve(ws);
    });

    ws.on('error', (error) => {
      console.error('‚ùå Deepgram WebSocket error:', error);
      reject(error);
    });

    ws.on('close', () => {
      console.log('üîå Deepgram WebSocket closed');
    });
  });
}

// Buffer for audio data while connecting
class AudioBuffer {
  constructor() {
    this.buffer = [];
    this.isConnecting = false;
  }

  add(data) {
    this.buffer.push(data);
  }

  clear() {
    this.buffer = [];
  }

  get data() {
    return this.buffer;
  }
}

// Update the WebSocket listener section
app.ws('/listen', async (plivoWs, req) => {
  console.log('üìû WebSocket /listen connected');
  let keepAliveInterval;
  let processingTimeout;
  let deepgramWs = null;
  const audioBuffer = new AudioBuffer();
  
  // Generate unique call ID
  const callId = `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  
  // Initialize conversation context
  await conversationManager.initializeContext(callId).catch(err => {
    console.error('‚ùå Failed to initialize conversation:', err);
    plivoWs.close();
    return;
  });

  // Function to connect to Deepgram
  const connectToDeepgram = async () => {
    if (audioBuffer.isConnecting) return;
    audioBuffer.isConnecting = true;

    try {
      deepgramWs = await initializeDeepgramWebSocket();
      
      // Send buffered audio
      if (audioBuffer.data.length > 0) {
        console.log('üì§ Sending buffered audio data...');
        for (const data of audioBuffer.data) {
          if (deepgramWs.readyState === WebSocket.OPEN) {
            deepgramWs.send(data);
          }
        }
        audioBuffer.clear();
      }
      
      // Set up Deepgram message handler
      deepgramWs.on('message', async (msg) => {
        try {
          const parsed = JSON.parse(msg.toString());
          
          if (parsed.type === 'Results' && parsed.channel?.alternatives?.length > 0) {
            const transcript = parsed.channel.alternatives[0].transcript;
            
            if (!transcript || transcript.trim() === '') return;
            
            console.log("üó£Ô∏è Transcribed:", transcript);
            
            const context = conversationManager.getContext(callId);
            if (!context) {
              console.error('‚ùå No context found for call:', callId);
              return;
            }

            context.transcriptBuffer += ' ' + transcript;
            
            if (textUtils.isEndOfThought(transcript, 1000)) {
              const fullUtterance = textUtils.cleanTranscript(context.transcriptBuffer);
              context.transcriptBuffer = '';

              if (textUtils.hasMinimumQuality(fullUtterance)) {
                console.log("ü§ñ Processing utterance:", fullUtterance);
                
                try {
                  // Call Hugging Face API with proper error handling
                  console.log("ü§ñ Calling Hugging Face API...");
                  const response = await fetch(HUGGING_FACE_API_URL, {
                    method: "POST",
                    headers: {
                      "Content-Type": "application/json",
                      "Authorization": `Bearer ${process.env.HUGGING_FACE_API_KEY}`
                    },
                    body: JSON.stringify({
                      inputs: `<s>[INST] ${fullUtterance} [/INST]`,
                      parameters: {
                        max_new_tokens: 100,
                        temperature: 0.7,
                        top_p: 0.9,
                        return_full_text: false,
                        do_sample: true
                      }
                    }),
                  });

                  if (!response.ok) {
                    throw new Error(`Hugging Face API error: ${response.status} - ${await response.text()}`);
                  }

                  const result = await response.json();
                  const aiResponse = result[0]?.generated_text || FALLBACK_RESPONSES[0];
                  console.log("ü§ñ AI Response:", aiResponse);
                  
                  // Use the new sendTTSResponse function
                  await sendTTSResponse(plivoWs, aiResponse);

                  // Store in database
                  await supabase.from('conversation_turns').insert([{
                    call_id: callId,
                    user_message: fullUtterance,
                    ai_response: aiResponse,
                    is_hugging_face: true,
                    timestamp: new Date().toISOString()
                  }]);

                } catch (error) {
                  console.error("‚ùå Hugging Face API error:", error);
                  // Use fallback response with better context
                  const fallbackResponse = FALLBACK_RESPONSES[Math.floor(Math.random() * FALLBACK_RESPONSES.length)];
                  console.log("‚ö†Ô∏è Using fallback response:", fallbackResponse);
                  
                  // Use the new sendTTSResponse function for fallback
                  await sendTTSResponse(plivoWs, fallbackResponse);
                }
              }
            }
          }
        } catch (error) {
          console.error('‚ùå Error processing Deepgram message:', error);
        }
      });

    } catch (error) {
      console.error('‚ùå Failed to connect to Deepgram:', error);
    } finally {
      audioBuffer.isConnecting = false;
    }
  };

  // Initial connection attempt
  await connectToDeepgram();

  // Handle Plivo messages
  plivoWs.on('message', async (msg) => {
    try {
      const parsed = JSON.parse(msg.toString());
      
      if (parsed.event === 'media' && parsed.media?.payload) {
        const audioData = Buffer.from(parsed.media.payload, 'base64');
        
        if (!deepgramWs || deepgramWs.readyState !== WebSocket.OPEN) {
          console.log('‚è≥ Buffering audio while connecting...');
          audioBuffer.add(audioData);
          if (!audioBuffer.isConnecting) {
            connectToDeepgram();
          }
        } else {
          try {
            deepgramWs.send(audioData);
          } catch (error) {
            console.error("‚ùå Error sending audio to Deepgram:", error);
            audioBuffer.add(audioData);
            connectToDeepgram();
          }
        }
      }
    } catch (error) {
      console.error('‚ùå Failed to process Plivo message:', error);
    }
  });

  // Clean up
  const cleanup = async () => {
    if (keepAliveInterval) clearInterval(keepAliveInterval);
    if (processingTimeout) clearTimeout(processingTimeout);
    if (deepgramWs && deepgramWs.readyState === WebSocket.OPEN) {
      deepgramWs.close();
    }
    await conversationManager.endConversation(callId);
  };

  plivoWs.on('close', cleanup);
});

// Keep Railway container alive with proper interval cleanup
let keepAliveInterval = setInterval(() => {}, 1000);

// Graceful shutdown
process.on('SIGTERM', () => {
  clearInterval(keepAliveInterval);
  wsInstance.getWss().clients.forEach(client => {
    client.close();
  });
  process.exit(0);
});

// Start server
app.listen(port, () => {
  console.log(`‚úÖ Deepgram WebSocket listener running on port ${port}...`);
});

